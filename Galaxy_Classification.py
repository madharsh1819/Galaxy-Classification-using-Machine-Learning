# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xkDkLdKzptNccQbBOTNaO17yuPS4_tyt
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns 
import cufflinks as cf
cf.go_offline()
# %matplotlib inline 


zoo = pd.read_csv('/content/drive/MyDrive/training_solutions_rev1 gz.csv')
zoo.head()

data = zoo.drop(['Class1.1','Class1.2','Class1.3','Class2.2','Class4.2','Class5.1','Class5.2','Class5.3','Class5.4','Class6.1','Class6.2','Class8.1','Class8.2','Class8.3','Class8.4','Class8.5','Class8.6','Class8.7','Class9.1','Class9.2','Class9.3','Class10.1','Class10.2','Class10.3','Class11.1','Class11.2','Class11.3','Class11.4','Class11.5','Class11.6'],axis=1)

y = data.drop(['Class7.1','Class7.2','Class7.3','Class2.1','Class4.1','Class3.1','Class3.2'],axis=1).values
X = data[['Class7.1','Class7.2','Class7.3','Class2.1','Class4.1','Class3.1','Class3.2']].values
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=101)
# normalising the data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""LogisticRegression"""

>>> from sklearn.datasets import load_iris
>>> from sklearn.linear_model import LogisticRegression
>>> X, y = load_iris(return_X_y=True)
>>> clf = LogisticRegression(random_state=0).fit(X, y)
>>> clf.predict(X[:2, :])

>>> clf.predict_proba(X[:2, :])

>>> clf.score(X, y)

"""Naiv baies"""

>>> from sklearn.model_selection import train_test_split
>>> from sklearn.naive_bayes import GaussianNB
>>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)
>>> gnb = GaussianNB()
>>> y_pred = gnb.fit(X_train, y_train).predict(X_test)
>>> print("Number of mislabeled points out of a total %d points : %d"
...       % (X_test.shape[0], (y_test != y_pred).sum()))

"""Stochastic Gradient Descent"""

>>> from sklearn.linear_model import SGDClassifier
>>> clf = SGDClassifier(loss="hinge", penalty="l2", max_iter=5)
>>> clf.fit(X, y)

>>> clf.predict([[100,100,100,100]])

>>> clf.coef_

>>> clf.intercept_

>>> clf.decision_function([[100,100,100,100]])

"""**KNeighborsClassifier**"""

>>> from sklearn.neighbors import KNeighborsClassifier
>>> neigh = KNeighborsClassifier(n_neighbors=3)
>>> neigh.fit(X, y)

>>> print(neigh.predict([[100,100,100,100]]))

>>> print(neigh.predict_proba([[100,100,100,100]]))

"""DecisionTreeClassifier"""

>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.tree import DecisionTreeClassifier
>>> clf = DecisionTreeClassifier(random_state=0)
>>> cross_val_score(clf, iris.data, iris.target, cv=10)

"""RandomForestClassifier"""

>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = RandomForestClassifier(max_depth=2, random_state=0)
>>> clf.fit(X, y)

>>> print(clf.predict([[0, 0, 0, 0]]))

"""Support vector machine"""

>>> from sklearn import svm
>>> clf = svm.SVC()
>>> clf.fit(X, y)

>>> clf.predict([[100,100,100,100]])

>>> # get support vectors
>>> clf.support_vectors_

>>> # get indices of support vectors
>>> clf.support_

>>> # get number of support vectors for each class
>>> clf.n_support_
